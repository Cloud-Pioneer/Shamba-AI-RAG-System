# IMPORTS
import json
import boto3
import time
import os
from boto3.dynamodb.conditions import Key

# AWS CLIENTS
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table("shamba-conversations")

bedrock_agent = boto3.client(
    "bedrock-agent-runtime",
    region_name="us-east-1"
)

bedrock_runtime = boto3.client(
    "bedrock-runtime",
    region_name="us-east-1"
)

# HELPER: FETCH MEMORY
def get_conversation_history(phone, limit=3):
    response = table.query(
        KeyConditionExpression=Key("phone").eq(phone),
        ScanIndexForward=False,
        Limit=limit
    )

    items = response.get("Items", [])
    items.reverse()
    return items

# HELPER: COST AWARENESS LOGGING
def log_cost_estimate(response_metadata):

    try:
        usage = response_metadata.get("usage", {})

        input_tokens = usage.get("inputTokens", 0)
        output_tokens = usage.get("outputTokens", 0)
        total_tokens = input_tokens + output_tokens

        # Approximate Claude 3 Haiku pricing
        # $0.25 per million input tokens
        # $1.25 per million output tokens

        input_cost = (input_tokens / 1_000_000) * 0.25
        output_cost = (output_tokens / 1_000_000) * 1.25
        total_cost = input_cost + output_cost

        print("TOKEN USAGE")
        print(f"Input Tokens: {input_tokens}")
        print(f"Output Tokens: {output_tokens}")
        print(f"Total Tokens: {total_tokens}")

        print("ESTIMATED COST")
        print(f"Estimated Request Cost: ${total_cost:.8f}")

    except Exception as e:
        print("Cost logging failed:", str(e))

# HELPER: DIRECT MODEL (Fallback)
def call_direct_model(prompt):

    response = bedrock_runtime.invoke_model(
        modelId="anthropic.claude-3-haiku-20240307-v1:0",
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 150,
            "temperature": 0.3,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        })
    )

    response_body = json.loads(response["body"].read())

    # Cost logging if available
    if "usage" in response_body:
        log_cost_estimate(response_body)

    return response_body["content"][0]["text"]

# MAIN HANDLER
def lambda_handler(event, context):

    print("Received event:", event)

    try:
        body = json.loads(event.get("body", "{}"))
        phone = body.get("phone")
        message = body.get("message")

        if not phone or not message:
            return {
                "statusCode": 400,
                "body": json.dumps({"error": "phone and message are required"})
            }

        # STEP 1: FETCH MEMORY
      
        history = get_conversation_history(phone)

        conversation_context = ""

        for item in history:
            conversation_context += f"Farmer: {item['message']}\n"
            conversation_context += f"Advisor: {item['reply']}\n"

      
        # STEP 2: BUILD PROMPT
        
        prompt = f"""
You are an agricultural advisor helping smallholder farmers in Kenya.

Use the knowledge base where relevant.
Keep answers practical and under 120 words.

Previous conversation:
{conversation_context}

Farmer question:
{message}
"""

        
        # STEP 3: TRY RAG FIRST
        
        try:
            response = bedrock_agent.retrieve_and_generate(
                input={"text": prompt},
                retrieveAndGenerateConfiguration={
                    "type": "KNOWLEDGE_BASE",
                    "knowledgeBaseConfiguration": {
                        "knowledgeBaseId": os.environ.get("KNOWLEDGE_BASE_ID")
                        "modelArn": os.environ.get("MODEL_ARN"), 
                    }
                }
            )

            reply = response["output"]["text"]

            # Log cost if usage metadata exists
            if "usage" in response:
                log_cost_estimate(response)

            # Fallback if KB failed
            if not reply or "unable to assist" in reply.lower():
                print("RAG returned empty result. Falling back to base model.")
                reply = call_direct_model(prompt)

        except Exception as rag_error:
            print("RAG failed:", str(rag_error))
            print("Falling back to base model.")
            reply = call_direct_model(prompt)

       # STEP 4: STORE CONVERSATION
      
        timestamp = str(int(time.time()))

        table.put_item(
            Item={
                "phone": phone,
                "timestamp": timestamp,
                "message": message,
                "reply": reply
            }
        )

       
        # STEP 5: RETURN RESPONSE
       
        return {
            "statusCode": 200,
            "headers": {
                "Content-Type": "application/json"
            },
            "body": json.dumps({
                "phone": phone,
                "reply": reply,
                "timestamp": timestamp
            })
        }

    except Exception as e:
        print("Unexpected error:", str(e))
        return {
            "statusCode": 500,
            "body": json.dumps({"error": "Internal server error"})
        }
